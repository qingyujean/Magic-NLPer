# Magic-NLPer

主要是对应博客中使用到的代码

博客主页：[https://blog.csdn.net/u010366748](https://blog.csdn.net/u010366748)

## 目录

- [机器学习](#机器学习)
- [深度学习](#深度学习)
- [自然语言处理](#自然语言处理)
- [论文阅读](#论文阅读)

## 机器学习

|博客地址 | 代码地址|
---|---
[线性回归（Linear Regression）原理小结](https://blog.csdn.net/u010366748/article/details/109545246)| [ML/LinearRegression线性回归/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/tree/main/MachineLearning/LinearRegression线性回归/t1.ipynb)
[逻辑斯蒂回归（logistic regression）原理小结](https://blog.csdn.net/u010366748/article/details/109552858)| [ML/LogisticRegression逻辑斯蒂回归/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/tree/main/MachineLearning/LogisticRegression逻辑斯蒂回归/t1.ipynb)
[最大熵（max entropy）模型原理小结](https://blog.csdn.net/u010366748/article/details/109628920)| 无
[决策树（Decision Tree）原理小结](https://blog.csdn.net/u010366748/article/details/109821147)| [ML/DecisionTree决策树/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/tree/main/MachineLearning/DecisionTree决策树/t1.ipynb)
[随机森林（Random Forest）原理小结](https://blog.csdn.net/u010366748/article/details/110008640)| [ML/RandomForest随机森林/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/tree/main/MachineLearning/RandomForest随机森林/t1.ipynb)
[梯度提升树（GBDT）原理小结](https://blog.csdn.net/u010366748/article/details/111060108)| [ML/GBDT梯度提升树/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/blob/main/MachineLearning/GBDT梯度提升树/t1.ipynb)
[XGBoost使用](https://blog.csdn.net/u010366748/article/details/111083706)| [ML/XGBoostUsage/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/blob/main/MachineLearning/XGBoostUsage/t1.ipynb)
[k近邻法（KNN）原理小结](https://blog.csdn.net/u010366748/article/details/112304969)| [ML/KNN/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/blob/main/MachineLearning/KNN/t1.ipynb)
[感知机（Perception）原理小结](https://blog.csdn.net/u010366748/article/details/112740411)| [ML/Perception感知机/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/blob/main/MachineLearning/Perception感知机/t1.ipynb)
[支持向量机（SVM）原理小结（1）线性支持向量机](https://blog.csdn.net/u010366748/article/details/112852999) <br>[支持向量机（SVM）原理小结（2）非线性支持向量机](https://blog.csdn.net/u010366748/article/details/113065986) <br>[支持向量机（SVM）原理小结（3）支持向量回归SVR](https://blog.csdn.net/u010366748/article/details/113066051)| [ML/SVM支持向量机/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/blob/main/MachineLearning/SVM支持向量机/t1.ipynb)
[朴素贝叶斯（naive bayes）原理小结](https://blog.csdn.net/u010366748/article/details/113150864)| [ML/NaiveBayes朴素贝叶斯/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/blob/main/MachineLearning/NaiveBayes朴素贝叶斯/t1.ipynb)
[EM（Expectation Maximization）算法原理小结](https://blog.csdn.net/u010366748/article/details/113446070)| 无
[隐马尔科夫模型（HMM）原理小结（1）](https://blog.csdn.net/u010366748/article/details/113554958) <br>[隐马尔科夫模型（HMM）原理小结（2）](https://blog.csdn.net/u010366748/article/details/113573732) <br>[手撸HMM实现词性标注（Part-of-speech）](https://blog.csdn.net/u010366748/article/details/113563529)| [ML/HMM隐马尔可夫模型/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/blob/main/MachineLearning/HMM隐马尔可夫模型/t1.ipynb)
[条件随机场（CRF）原理小结（1）](https://blog.csdn.net/u010366748/article/details/113781150) <br>[条件随机场（CRF）原理小结（2）](https://blog.csdn.net/u010366748/article/details/113783526) <br>[BiLSTM-CRF实现中文命名实体识别（NER）](https://blog.csdn.net/u010366748/article/details/113784204)| [ML/CRF条件随机场/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/blob/main/MachineLearning/CRF条件随机场/t1.ipynb)
[集成学习原理小结（AdaBoost & lightGBM demo）](https://blog.csdn.net/u010366748/article/details/113816465)| [ML/AdaBoost/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/tree/main/MachineLearning/AdaBoost/t1.ipynb) <br>[ML/AdaBoost/lightgbm_demo.ipynb](https://github.com/qingyujean/Magic-NLPer/tree/main/MachineLearning/AdaBoost/lightgbm_demo.ipynb)
[统计机器学习相关概念总结（上）](https://blog.csdn.net/u010366748/article/details/113829064) <br>[统计机器学习相关概念总结（中）](https://blog.csdn.net/u010366748/article/details/113829373) <br>[统计机器学习相关概念总结（下）](https://blog.csdn.net/u010366748/article/details/113829508)| 无

## 深度学习
|博客地址 | 代码地址|
---|---
[transformer（上）论文解读+pytorch实现](https://blog.csdn.net/u010366748/article/details/111183674) <br>[transformer（下）机器翻译+pytorch实现](https://blog.csdn.net/u010366748/article/details/111269231)| [DL/Transformer/MachinTranslation/pytorch/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/blob/main/DeepLearning/Transformer/MachinTranslation/pytorch/t1.ipynb)
[条件随机场（CRF）原理小结（1）](https://blog.csdn.net/u010366748/article/details/113781150) <br>[条件随机场（CRF）原理小结（2）](https://blog.csdn.net/u010366748/article/details/113783526) <br>[BiLSTM-CRF实现中文命名实体识别（NER）](https://blog.csdn.net/u010366748/article/details/113784204)| [ML/CRF条件随机场/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/blob/main/MachineLearning/CRF条件随机场/t1.ipynb)

## 自然语言处理
|博客地址 | 代码地址|
---|---
[transformer（上）论文解读+pytorch实现](https://blog.csdn.net/u010366748/article/details/111183674) <br>[transformer（下）机器翻译+pytorch实现](https://blog.csdn.net/u010366748/article/details/111269231)| [NLP/MachineTranslation/transformer/pytorch/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/tree/main/NLP/MachineTranslation/transformer/pytorch/t1.ipynb)
[朴素贝叶斯（naive bayes）原理小结](https://blog.csdn.net/u010366748/article/details/113150864)| [NLP/Classification/binary/naive_bayes/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/tree/main/NLP/Classification/binary/naive_bayes/t1.ipynb)
[隐马尔科夫模型（HMM）原理小结（1）](https://blog.csdn.net/u010366748/article/details/113554958) <br>[隐马尔科夫模型（HMM）原理小结（2）](https://blog.csdn.net/u010366748/article/details/113573732) <br>[手撸HMM实现词性标注（Part-of-speech）](https://blog.csdn.net/u010366748/article/details/113563529)| [ML/HMM隐马尔可夫模型/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/blob/main/MachineLearning/HMM隐马尔可夫模型/t1.ipynb)
[条件随机场（CRF）原理小结（1）](https://blog.csdn.net/u010366748/article/details/113781150) <br>[条件随机场（CRF）原理小结（2）](https://blog.csdn.net/u010366748/article/details/113783526) <br>[BiLSTM-CRF实现中文命名实体识别（NER）](https://blog.csdn.net/u010366748/article/details/113784204)| [ML/CRF条件随机场/t1.ipynb](https://github.com/qingyujean/Magic-NLPer/blob/main/MachineLearning/CRF条件随机场/t1.ipynb)

## 论文阅读
|论文 | 发表年份|  博客地址 |
---|---|---
[Character-Level Language Modeling with Deeper Self-Attention](https://arxiv.org/abs/1808.04444) | AAAI 2019 | [论文笔记-Vanilla Transformer](https://blog.csdn.net/u010366748/article/details/114301942)|
[Deep contextualized word representations](https://arxiv.org/abs/1802.05365) | 	NAACL 2018 | [ELMo论文笔记+源码分析](https://blog.csdn.net/u010366748/article/details/110309131)|
[Attention Is All You Need](https://arxiv.org/abs/1706.03762) | NeurIPS 2017 | [transformer（上）论文解读+pytorch实现](https://blog.csdn.net/u010366748/article/details/111183674)|
[Neural entity linking: A survey of models based on deep learning](https://arxiv.org/abs/2006.00575) | 2020 | [实体链指（1）综述](https://blog.csdn.net/u010366748/article/details/126680220)|
[Neural Cross-Lingual Entity Linking](https://arxiv.org/abs/1712.01813) | AAAI, 2018 | [实体链指（2）EL：Disambiguation-Only](https://blog.csdn.net/u010366748/article/details/126686525)|
[Scalable Zero-shot Entity Linking with Dense Entity Retrieval](https://arxiv.org/abs/1911.03814) | EMNLP, 2020 | [实体链指（2）EL：Disambiguation-Only](https://blog.csdn.net/u010366748/article/details/126686525)|
[End-to-End Neural Entity Linking](https://arxiv.org/abs/1808.07699) | CoNLL 2018 | [实体链指（3）EL：End-to-End](https://blog.csdn.net/u010366748/article/details/126691127)|
[Efficient One-Pass End-to-End Entity Linking for Questions](https://arxiv.org/abs/2010.02413) | EMNLP 2020 | [实体链指（3）EL：End-to-End](https://blog.csdn.net/u010366748/article/details/126691127)|